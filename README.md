# Awesome-Quantization-Papers-For-LLM
A collection of papers on quantization techniques for large language models, compiled for easy reference and personal study.

## Overview

- "Evaluating Quantized Large Language Models", ICML, 2024. [[paper](https://openreview.net/forum?id=DKKg5EFAFr)] [**`Survey`**] [**`None`**]  [**`None`**] 
- "SqueezeLLM: Dense-and-Sparse Quantization", ICML, 2024. [[paper](https://openreview.net/forum?id=0jpbpFia8m)] [**`W`**] [**`No Fine-tuning`**] [**`Non-uniform`**]
- "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache", ICML, 2024. [[paper](https://openreview.net/forum?id=L057s2Rq8O)] [**`KV`**] [**`No Fine-tuning`**] [**`Uniform`**]
- "Extreme Compression of Large Language Models via Additive Quantization", ICML, 2024. [[paper](https://openreview.net/forum?id=5mCaITRTmO)] [**`W`**] [**`to be fill`**]  [**`to be fill`**]
- "BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization", ICML, 2024. [[paper](https://openreview.net/forum?id=DbyHDYslM7)] [**`WA`**] [**`to be fill`**]  [**`to be fill`**]
- "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models", ICLR, 2024. [[paper](https://openreview.net/forum?id=8Wuvhh0LYW)] [[code](https://github.com/OpenGVLab/OmniQuant/tree/main)] [**`WA`**] [**`Fine-tuning`**] [**`Uniform`**]
- "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models", ICLR, 2024. [[paper](https://openreview.net/forum?id=LzPWWPAdY4)]  [**`W`**] [**`Fine-tuning`**] [**`Uniform`**]
- "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", ICML, 2023. [[paper](https://arxiv.org/abs/2211.10438)] [[code](https://github.com/mit-han-lab/smoothquant)] [**`WA`**] [**`No Fine-tuning`**] [**`Uniform`**]
- "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", ICLR, 2023. [[papar](https://arxiv.org/abs/2210.17323)] [[code](https://github.com/IST-DASLab/gptq)] [**`W`**] [**`to be fill`**] [**`Uniform`**]
